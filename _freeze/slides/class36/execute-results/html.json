{
  "hash": "5dea3ad1b88ed45f93d2410e18c1eebc",
  "result": {
    "markdown": "---\ntitle: \"Review of Empirical Exercises\"\nauthor: \"Ian McCarthy | Emory University\"\nformat: \n  revealjs:\n    theme: [moon]\n    preview-links: auto\n#    chalkboard:\n#      boardmarker-width: 5\n    slide-number: true\n    width: 1600\n    height: 900    \n    embed-resources: true\nfrom: markdown+emoji\nexecute: \n  echo: true\n---\n\n\n\n\n\n## Areas of empirical work\n\nEmpirical excercises cover four \"applied\" areas:\n\n1. Difference-in-differences\n2. Instrumental variables\n3. Regression discontinuity\n4. Demand estimation and hospital market construction\n\n::: {.fragment}\nToday, let's talk about each of these (briefly). And provide some code examples/hints.\n:::\n\n---\n\n## Workflow \n\nAnother sub-goal of the exercises is to practice good workflow. I'll focus on two possible workflows:\n\n1. \"Solo-author\" workflow, where everything lives on your computer, Git/GitHub, and **nothing** is hard-coded\n2. \"Co-author\" workflow, where you transition to Overleaf for writing purposes\n\n\n# Exercise 1: Difference-in-differences\n\n---\n\n## Question\n\nDoes Medicaid expansion reduced hospital uncompensated care costs?\n\n---\n\n## Data\n\n1. Hospital Cost Report Information System\n2. Provider of Services files\n3. Medicaid expansion from KFF\n\n---\n\n## Data management\n\n\n- Create hcris, pos, and KFF medicaid datasets separately\n- Join appropriately\n- Define expansion period and uncompensated care costs\n- Trim data to remove outliers (many, many different/better ways to do this)\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R Code\"}\nfull.data <- hcris.data %>% rename(state_short=state) %>%\n  left_join(pos.data %>% select(provider_number=provider, category, own_change, beds_cert, beds_tot, name,\n                                term_date, fac_type, own_type, category_sub, profit_status, year), \n            by=c(\"provider_number\", \"year\")) %>%\n  left_join(medicaid.data, by=\"state_short\") %>%\n  mutate(profit_status = \n           case_when(\n             own_type==\"Non-profit Private\" ~ \"Non Profit\",\n             own_type %in% c(\"Physician Owned\",\"Profit\") ~ \"For Profit\"\n           )) %>%\n  mutate(expand_year=year(date_adopted),\n         expand = (year>=expand_year & !is.na(expand_year)),\n         uncomp_care=uncomp_care*cost_to_charge) %>%\n  rename(expand_ever=expanded) %>%\n  filter(year>=2003, year<2020,\n         uncomp_care>0, tot_pat_rev>0)\n\ntrim.data <- full.data %>%\n  group_by(year) %>%\n  mutate(ptile_uncomp=ntile(uncomp_care,100)) %>%\n  filter(ptile_uncomp>1 & ptile_uncomp<99)\n```\n:::\n\n\n\n---\n\n## Identification strategy (ies)\n\n- DD with staggered treatment adoption (Medicaid expansion)\n- Work through different specifications/strategies:\n  - TWFE by group and with staggered treatment\n  - Event study by group and with staggered treatment\n  - Sun and Abraham with event study\n  - Callaway and Sant'Anna with event study\n  - Rambachan and Roth \"honest DD\"\n\n---\n\n## Estimation\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"TWFE\"}\nreg.data1 <- trim.data %>% mutate(uncomp_care=uncomp_care/1000) %>%\n  mutate(treat=\n           case_when(\n             year>=expand_year & !is.na(expand_year) ~ 1,\n             year<expand_year & !is.na(expand_year) ~0,\n             is.na(expand_year) ~ 0\n           )\n         )\n\nreg.data2 <- trim.data %>% mutate(uncomp_care=uncomp_care/1000) %>%\n  filter(is.na(expand_year) | expand_year==2014) %>%\n  mutate(post=(year>=2014),\n         treat=post*expand_ever)\n\nreg.data3 <- trim.data %>% mutate(uncomp_care=uncomp_care/1000) %>%\n  filter(is.na(expand_year) | expand_year==2015) %>%\n  mutate(post=(year>=2015),\n         treat=post*expand_ever)\n\nreg.data4 <- trim.data %>% mutate(uncomp_care=uncomp_care/1000) %>%\n  filter(is.na(expand_year) | expand_year==2016) %>%\n  mutate(post=(year>=2016),\n         treat=post*expand_ever)\n\ndd.est1 <- feols(uncomp_care~treat | year + provider_number, data=reg.data1)\ndd.est2 <- feols(uncomp_care~treat | year + provider_number, data=reg.data2)\ndd.est3 <- feols(uncomp_care~treat | year + provider_number, data=reg.data3)\ndd.est4 <- feols(uncomp_care~treat | year + provider_number, data=reg.data4)\n\nsum.fmt <- function(x) formatC(x, digits = 2, big.mark = \",\", format = \"f\")\ndd.summary <- msummary(list(\"Full Sample\"=dd.est1, \"Expand 2014\"=dd.est2, \n                            \"Expand 2015\"=dd.est3, \"Expand 2016\"=dd.est4),\n                       shape=term + statistic ~ model, \n                       gof_map=NA,\n                       coef_omit='Intercept',\n                       coef_rename=c(\"treat\"=\"Expansion\"),\n                       fmt= sum.fmt,\n                       vcov = ~provider_number,\n                       output=\"markdown\",\n                       caption=\"TWFE Estimates for Different Treatment Groups\",\n                       label=\"ddmodels\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Event Studies\"}\n## based on full data\nevent.data1 <- reg.data1 %>%\n  mutate(event_time=\n           case_when(\n             !is.na(expand_year) ~ year-expand_year,\n             is.na(expand_year) ~ -1\n           )\n         ) \n\nevent.reg1 <- feols(uncomp_care ~ i(as.factor(event_time), expand_ever, ref=-1) | year + provider_number, \n                   cluster=~provider_number, data=event.data1)\n\n\n\n## based on 2014 treatment group only\nevent.reg2 <- feols(uncomp_care ~ i(as.factor(year), expand_ever, ref=2013) | year + provider_number, \n                    cluster=~provider_number, data=reg.data2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Sun and Abraham\"}\nsa.data <- event.data1 %>%\n  mutate(expand_year = ifelse(expand_ever==FALSE, 10000, expand_year),\n         time_to_treat = ifelse(expand_ever==FALSE, -1, year-expand_year),\n         time_to_treat = ifelse(time_to_treat < -15, -15, time_to_treat))\n\nsa.reg <- feols(uncomp_care ~ sunab(expand_year, time_to_treat) | year + provider_number,\n                cluster=~provider_number, data=sa.data)\nsa.est <- tidy(summary(sa.reg, agg=FALSE)) %>%\n  filter(str_detect(term,\"cohort::2014|cohort::2015|cohort::2016\")) %>%\n  mutate(term=str_replace(term,\"time_to_treat::\",\"\"),\n         term=str_replace(term,\":cohort::\",\":\")) %>%\n  separate(term,c(\"period\",\"cohort\"),\":\") %>%\n  mutate(period=as.numeric(period)) %>%\n  select(period, cohort, estimate, p.value) %>%\n  rename(p_value=p.value) \n\nsa.event.plot <- iplot(sa.reg, xlab = \"Time to Treatment\", main=\"Sun and Abraham Event Study\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Callaway and Sant'Anna\"}\ncs.data <- reg.data1 %>%\n  mutate(expand_year=ifelse(is.na(expand_year),0,expand_year)) %>%\n  group_by(provider_number) %>%\n  mutate(hospital_id=cur_group_id()) %>% ungroup()\n\n\ncs.mod <- att_gt(yname=\"uncomp_care\", tname=\"year\", idname=\"hospital_id\",\n                 gname=\"expand_year\",\n                 data=cs.data, panel=TRUE, est_method=\"dr\",\n                 allow_unbalanced_panel=TRUE)\ncs.event <- aggte(cs.mod, type=\"dynamic\")\n\ncoef.cs <- tidy(cs.event) %>%\n  select(rel_year=event.time, estimate, ci_lower=conf.low, ci_upper=conf.high) %>%\n  mutate(rel_year=as.numeric(rel_year))\ncoef.cs <- as_tibble(coef.cs)\n\ncs.plot <- ggplot(coef.cs, aes(x=rel_year, y=estimate)) + \n  geom_point(size = 2)  +\n  geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = -0.5, linetype = \"dashed\") +\n  theme(legend.position=\"none\") +\n  scale_x_continuous(breaks = -15:5, minor_breaks = NULL) +\n  scale_y_continuous(minor_breaks = NULL, label=comma) +\n  labs(x = \"Relative Time\", y = \"Estimated Effect, $1000s\", color = NULL, title = NULL) +\n  theme_bw()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Rambachan and Roth Honest DD\"}\nlibrary(devtools)\ninstall_github(\"bcallaway11/BMisc\", dependencies = TRUE)\ninstall_github(\"asheshrambachan/HonestDiD\", dependencies = TRUE)\n\nlibrary(HonestDiD)\nsource('solutions/exercise1/fn_honest_did.R')\n\ncs.hd <- att_gt(yname=\"uncomp_care\", tname=\"year\", idname=\"hospital_id\",\n                 gname=\"expand_year\",\n                 data=cs.data, panel=TRUE, est_method=\"dr\",\n                 allow_unbalanced_panel=TRUE,\n                base_period=\"universal\")\ncs.hd.event <- aggte(cs.hd, type=\"dynamic\", min_e=-10, max_e=5)\n\nhd.cs <- honest_did(cs.hd.event, type=\"smoothness\", Mvec=seq(from=0, to=2000, by=500))\nhd.cs.graph <- createSensitivityPlot(hd.cs$robust_ci,\n                                        hd.cs$orig_ci)\n\ncoef.cs.hd <- hd.cs$robust_ci %>% bind_rows(hd.cs$orig_ci) %>%\n  mutate(type=case_when(\n    M==2000 ~ \"M = +2,000\",\n    M==1500 ~ \"M = +1,500\",\n    M==1000 ~ \"M = +1,000\",\n    M==500 ~ \"M = +500\",\n    M==0 ~ \"M = 0\",\n    is.na(M) ~ \"Original\"\n  ))\n\ncs.hd.plot <- ggplot(coef.cs.hd, aes(x=factor(type, \n                                              level=c('Original', 'M = 0', 'M = +500', 'M = +1,000', 'M = +1,500', 'M = +2,000')))) + \n  geom_linerange(aes(ymin = lb, ymax = ub)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  theme(legend.position=\"none\") +\n  scale_y_continuous(minor_breaks = NULL, label=comma) +\n  labs(x = \"Violation in Parallel Trends\", y = \"Estimated Effect (at t=0)\", color = NULL, title = NULL) +\n  theme_bw()\n```\n:::\n\n\n\n\n# Exercise 2: Instrumental Variables\n\n---\n\n## Question\n\nWhat is the effect of vertical integration on total physician billable activity?\n\n\n---\n\n## Data\n\n1. Medicare Data on Provider and Practice Specialty (MD-PPAS)\n2. Medicare Utilization and Payment Data (PUF)\n3. Physician Fee Schedule (PFS)...already provided for you\n\n---\n\n## Data management\n\n- Using MD-PPAS, PFS, and PUF...calculate physician relative price changes due to price shock\n- Aggregate to practice level\n- Repeat 100 times for random realizations of price shock\n- Recenter practice level price changes based on random realizations\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R Code\"}\npfs.update <- read_tsv(\"solutions/exercise2/data/input/PFS_update_data.txt\")\ntaxid.base <- read_csv(\"solutions/exercise2/data/input/MDPPAS/PhysicianData_2009.csv\") %>%\n  select(npi, tax_id_base=group1)\n\n## for recentering (part 8)\npfs.year <- pfs.update %>% filter(year==2012)\nvalue <- 1:100\nn.rows=nrow(pfs.year)\npfs.year.rc <- bind_cols(pfs.year,\n                           map_dfc(value, ~pfs.year %>% transmute(!!paste0('dprice_rel_2010_', .x) := sample(dprice_rel_2010, n.rows)))) %>%\n  select(hcpcs, starts_with('dprice_rel_2010_'))\n##\n\nstep <- 0\nfor (i in 2012:2017) {\n  step <- step+1\n  \n  if (i<=2013) {\n    pfs.yearly <- pfs.update %>% filter(year==i)\n  } else {\n    pfs.yearly <- pfs.update %>% filter(year==2013)\n  }\n  \n  \n  mdppas <- read_csv(paste0(\"solutions/exercise2/data/input/MDPPAS/PhysicianData_\",i,\".csv\"))\n  \n  medicare.puf <- read_delim(paste0(\"solutions/exercise2/data/input/utilization-payment-puf/\",i,\"/Medicare_Provider_Util_Payment_PUF_CY\",i,\".txt\")) %>%\n    mutate(year=i) %>%\n    rename_with(tolower) %>%\n    filter(str_detect(nppes_credentials, \"M.D.|MD|m.d.|md\")) %>%\n    mutate(npi=as.numeric(npi),\n           spend=average_medicare_allowed_amt*bene_day_srvc_cnt)\n\n  price.shock <- medicare.puf %>% inner_join(taxid.base, by=\"npi\") %>%\n    inner_join(pfs.yearly %>% select(hcpcs, dprice_rel_2010, price_nonfac_orig_2010, price_nonfac_orig_2007), by=c(\"hcpcs_code\"=\"hcpcs\")) %>%\n    mutate_at(vars(dprice_rel_2010, price_nonfac_orig_2010, price_nonfac_orig_2007), replace_na, 0) %>%\n    mutate(price_shock = case_when(\n            i<=2013 ~ ((i-2009)/4)*dprice_rel_2010,\n            i>2013  ~ dprice_rel_2010),\n          denom = line_srvc_cnt*price_nonfac_orig_2010,\n          numer = price_shock*line_srvc_cnt*price_nonfac_orig_2010) %>%\n    group_by(npi) %>%\n    summarize(phy_numer=sum(numer, na.rm=TRUE), phy_denom=sum(denom, na.rm=TRUE), tax_id_base=first(tax_id_base)) %>%\n    ungroup() %>%\n    mutate(phy_rev_change=phy_numer/phy_denom)\n\n  price.shock.practice <- price.shock %>%\n    group_by(tax_id_base) %>%\n    summarize(practice_rev_change=sum(phy_rev_change, na.rm=TRUE)) %>%\n    ungroup()\n  \n  price.shock.rc <- medicare.puf %>% inner_join(taxid.base, by=\"npi\") %>%\n    inner_join(pfs.yearly %>% select(hcpcs, price_nonfac_orig_2010), by=c(\"hcpcs_code\"=\"hcpcs\")) %>%        \n    inner_join(pfs.year.rc, by=c(\"hcpcs_code\"=\"hcpcs\")) %>%\n    mutate(across(starts_with('dprice_rel_2010'), replace_na, 0)) %>%\n    mutate(across(starts_with('dprice_rel_2010'),\n                  ~ case_when(\n                    i<=2013 ~ ((i-2009)/4)*(.x),\n                    i>2013 ~ .x),\n                  .names = \"price_shock{sub('dprice_rel_2010_','_',col)}\"),\n           denom = line_srvc_cnt*price_nonfac_orig_2010,\n           across(starts_with('price_shock'),\n                  ~ (.x)*line_srvc_cnt*price_nonfac_orig_2010,\n                  .names = \"numer{sub('price_shock_','_',col)}\")) %>%\n    group_by(npi) %>%\n    summarize(across(starts_with('numer'),\n                     ~ sum( .x, na.rm=TRUE),\n                     .names = \"phy_{col}\"),\n              phy_denom=sum(denom, na.rm=TRUE),\n              tax_id_base=first(tax_id_base)) %>%\n    ungroup() %>%\n    mutate(across(starts_with('phy_numer'),\n                  ~ (.x)/phy_denom,\n                  .names = \"phy_rev_change{sub('phy_numer_','_',col)}\"))\n\n  price.shock.practice.rc <- price.shock.rc %>%\n    group_by(tax_id_base) %>%\n    summarize(across(starts_with('phy_rev_change'),\n                     ~ sum(.x, na.rm=TRUE),\n                     .names = \"{sub('phy','practice',col)}\")) %>%\n    ungroup() %>%\n    mutate(mean_practice_shock=rowMeans( across(starts_with('practice_rev_change_')), na.rm=TRUE)) %>%\n    select(tax_id_base, mean_practice_shock)\n\n  phy.data <- medicare.puf %>% \n    group_by(npi) %>%\n    summarize(total_claims=sum(bene_day_srvc_cnt, na.rm=TRUE),\n              total_spend=sum(spend, na.rm=TRUE),\n              total_patients=sum(bene_unique_cnt, na.rm=TRUE)) %>%\n    inner_join(taxid.base, by=\"npi\") %>%\n    inner_join(mdppas %>% select(npi, group1, group2, pos_opd, pos_office, pos_asc), by=\"npi\") %>%\n    inner_join(price.shock %>% select(-tax_id_base), by=\"npi\") %>%\n    inner_join(price.shock.practice, by=\"tax_id_base\")  %>%\n    inner_join(price.shock.practice.rc, by=\"tax_id_base\")  %>%\n    rename(tax_id_2009=tax_id_base) %>%\n    mutate(year=i) %>%\n    ungroup()\n  \n  if (step==1) {\n    final.data <- phy.data\n  } else {\n    final.data <- bind_rows(final.data, phy.data)\n  }\n}\n```\n:::\n\n\n\n---\n\n## Identification strategy\n\n- First, assess potential role of selection on unobservables\n- Exploit PPIS price shock as instrument for integration\n  - Consider new work on instrument strength\n  - Consider \"recentering\" as proposed by Borusyak and Hull\n\n---\n\n## Estimation\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Role of selection on observables\"}\ndelta.dx <- feols(ln_claims ~ int | npi + year, data=reg.data)\ndelta.d <- feols(ln_claims ~ int, data=reg.data)\n\nhat.dx <- tidy(delta.dx) %>% select(estimate)\nhat.d <- tidy(delta.d) %>% filter(term=='int1') %>% select(estimate)\nR.dx <- glance(delta.dx)$r.squared\nR.d <- glance(delta.d)$r.squared\noster.data <- tibble(\n  coef_dx = hat.dx$estimate,\n  coef_d = hat.d$estimate,\n  r_dx = R.dx,\n  r_d = R.d\n)\n\nrho <- tibble(\n  rho= c(0,.5,1,1.5,2),\n)\n\nr.max <- tibble(\n  r_max= c(0.5, 0.6, 0.7, 0.8, 0.9, 1)\n)\n\noster.bound <- crossing(rho, r.max) %>%\n  bind_cols(oster.data) %>%\n  mutate(d_upper = coef_dx - rho*(coef_d - coef_dx)*( (r_max - r_dx)/(r_dx - r_d)))\n\noster.table <- pivot_wider(oster.bound %>% select(rho, r_max, d_upper),\n            names_from=rho,\n            names_glue=\"rho_{rho}\",\n            values_from=d_upper)\n\nkable(oster.table, format=\"latex\",\n      col.names=c(\"Max R2\",\"Rho 0\",\"Rho 0.5\", \"Rho 1.0\", \"Rho 1.5\", \"Rho 2.0\"),\n      digits=c(1,3,3,3,3,3),\n      format.args=list(big.mark=\",\"),\n      booktabs=T) %>%\n  kable_styling(latex_options=c(\"HOLD_position\")) %>%\n  save_kable(\"solutions/exercise2/t3_oster.tex\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"IV Estimation\"}\nmod.fs <- feols(as.numeric(int) ~ practice_rev_change | npi + year, data=reg.data)\nmod.iv <- feols(ln_claims ~ 0 | npi + year | int ~ practice_rev_change, data=reg.data)\nmod.rf <- feols(ln_claims ~ practice_rev_change | npi + year, data=reg.data)\n\niv.summary <- msummary(list(\"OLS\"=mod.ols, \"IV\"=mod.iv, \n                            \"Reduced Form\"=mod.rf, \"First Stage\"=mod.fs),\n                       shape=term + statistic ~ model, \n                       gof_map=NA,\n                       coef_omit='Intercept',\n                       coef_rename=c(\"int1\"=\"Integration\",\n                                     \"fit_int1\"=\"Integration\",\n                                     \"practice_rev_change\"=\"Instrument\"),\n                       vcov = ~npi,\n                       caption=\"Instrumental Variables Estimates\",\n                       output=\"solutions/exercise2/t4-iv.tex\",\n                       label=\"ivmodels\",\n                       booktabs=T) %>%\n  kable_styling(latex_options='HOLD_position')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Weak Instruments\"}\nmod.fe1 <- feols(as.numeric(int) ~ 1 | npi + year, data=reg.data)\nmod.fe2 <- feols(practice_rev_change ~ 1 | npi + year, data=reg.data)\nmod.fe3 <- feols(ln_claims ~ 1 | npi + year, data=reg.data)\n\nar.data <- reg.data %>%\n  filter(!is.na(npi)) %>%\n  add_residuals(mod.fe1, var=\"x_hat\") %>%\n  add_residuals(mod.fe2, var=\"z_hat\") %>%\n  add_residuals(mod.fe3, var=\"y_hat\") %>%\n  select(y_hat, x_hat, z_hat, npi, year)\n\nar.data <- as.data.frame(ar.data)\nY=ar.data[,\"y_hat\"]\nD=ar.data[,\"x_hat\"]\nZ=ar.data[,\"z_hat\"]\nweak.test <- ivmodel(Y=Y, D=D, Z=Z)\nkable(weak.test$AR$ci, format=\"latex\",\n      col.names=c(\"Lower CI\",\"Upper CI\"),\n      digits=c(3,3),\n      booktabs=T) %>%\n  kable_styling(latex_options=c(\"HOLD_position\")) %>%\n  save_kable(\"solutions/exercise2/t6_ar.tex\")\n\n## still need Lee et al tF stat\nlee.tf <- msummary(list(\"IV\"=mod.iv, \"First Stage\"=mod.fs),\n                        shape=term + statistic ~ model, \n                        gof_map=NA,\n                        coef_omit='Intercept',\n                        coef_rename=c(\"int1\"=\"Integration\",\n                                      \"fit_int1\"=\"Integration\",\n                                      \"practice_rev_change\"=\"Instrument\"),\n                        vcov = ~npi,\n                        statistic=\"statistic\",\n                        caption=\"Instrumental Variables Estimates\",\n                        output=\"solutions/exercise2/t6-tf.tex\",\n                        label=\"ivmodels\",\n                        booktabs=T) %>%\n  kable_styling(latex_options='HOLD_position')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Recentering\"}\nreg.data <- reg.data %>%\n  mutate(mean_shock=mean_practice_shock/1000) %>%\n  mutate(shock_rc=practice_rev_change-mean_shock)\n\nmod.fs2 <- feols(as.numeric(int) ~ shock_rc | npi + year, data=reg.data)\nmod.iv2 <- feols(ln_claims ~ 0 | npi + year | int ~ shock_rc, data=reg.data)\nmod.rf2 <- feols(ln_claims ~ shock_rc | npi + year, data=reg.data)\n\niv.summary2 <- msummary(list(\"OLS\"=mod.ols, \"IV\"=mod.iv2, \n                            \"Reduced Form\"=mod.rf2, \"First Stage\"=mod.fs2),\n                       shape=term + statistic ~ model, \n                       gof_map=NA,\n                       coef_omit='Intercept',\n                       coef_rename=c(\"int1\"=\"Integration\",\n                                     \"fit_int1\"=\"Integration\",\n                                     \"shock_rc\"=\"Recentered Instrument\"),\n                       vcov = ~npi,\n                       caption=\"Instrumental Variables Estimates\",\n                       output=\"solutions/exercise2/t7-iv2.tex\",\n                       label=\"ivmodels\",\n                       booktabs=T) %>%\n  kable_styling(latex_options='HOLD_position')\n```\n:::\n\n\n\n\n# Exercise 3: Regression Discontinuity\n\n---\n\n## Question\n\nWhat is the effect of the low income subsidy on Part D enrollment?\n\n\n---\n\n## Data\n\n1. Replication data from Ericson (2014)\n2. Based on Part D enrollment and low-income subsidity data (publicly available)\n\n---\n\n## Data management\n\nJust combining datasets from replication files\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"R Code\"}\n# calculate shares from pdp data\npdp.data <- pdp.data %>%\n  group_by(state, year) %>%\n  mutate(state_yr_enroll = sum(enrollment, na.rm=TRUE)) %>%\n  ungroup() %>%\n  mutate(share = enrollment/state_yr_enroll,\n         ln_share = log(share))\n\n# reshape subsidy data to long\nlis.data <- pivot_longer(lis.data, cols=c(\"s2006\",\"s2007\",\"s2008\",\"s2009\",\"s2010\"), \n                         names_to=\"year\",\n                         names_prefix=\"s\",\n                         values_to=\"LISsubsidy\") %>%\n  mutate(year=as.numeric(year))\n\n# merge the subsidy data into the pdp data and create new variables matching Ericson's code files\nfinal.data <- pdp.data %>%\n  inner_join(lis.data, by=c(\"PDPregion\",\"year\")) %>%\n  mutate(LISPremium = premium - LISsubsidy,\n         proposedBenchmarkPlan = ifelse(LISPremium<=0,1,0),\n         ProblemObs = case_when(\n           LISPremium < 0 & LIS == 0 ~ 1,\n           LISPremium > 0 & LIS == 1 ~ 2\n         ),\n         LISPremium = ifelse(benefit==\"E\",NA,LISPremium),\n         proposedBenchmarkPlan = ifelse(benefit==\"E\", NA, proposedBenchmarkPlan),\n         LISPremiumNeg = ifelse(LISPremium<=0, LISPremium, 0),\n         LISPremiumPos = ifelse(LISPremium>=0, LISPremium, 0))\n\n# recreate Ericson RD windows\nfinal.data.rd <- final.data %>%\n  mutate(RDwindow1 = ifelse(LISPremium>=-10 & LISPremium<=10 & year==2006, 1, 0),\n         belowBench1 = ifelse(LISPremium<=0 & RDwindow1==1, 1, 0),\n         RDwindow2 = ifelse(LISPremium>=-4 & LISPremium<=4 & year==2006, 1, 0),\n         belowBench2 = ifelse(LISPremium<=0 & RDwindow2==1, 1, 0),\n         RDwindow3 = ifelse(LISPremium>=-2.5 & LISPremium<=2.5 & year==2006, 1, 0),\n         belowBench3 = ifelse(LISPremium<=0 & RDwindow3==1, 1, 0),\n         RDwindow4 = ifelse(LISPremium>=-6 & LISPremium<=6 & year==2006, 1, 0),\n         belowBench4 = ifelse(LISPremium<=0 & RDwindow4==1, 1, 0)) %>%\n  select(uniqueID, starts_with(c(\"RDwindow\",\"belowBench\"))) %>%\n  group_by(uniqueID) %>%\n  summarize_all(max, na.rm=TRUE) %>%\n  filter_at(vars(RDwindow1:belowBench4), all_vars(is.finite(.)))\n\nfinal.data <- final.data %>%\n  left_join(final.data.rd, by=c(\"uniqueID\"))\n```\n:::\n\n\n\n---\n\n## Identification strategy\n\n- Binned scatterplots\n- RD estimation\n  - Manipulation of the running variable\n  - Optimal bandwidth and inference\n- Extension of Ericson (2014) using IV\n\n---\n\n## Estimation\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Binned Scatterplots\"}\nfinal.bw1 <- final.data %>% \n  filter(RDwindow1==1, benefit==\"B\", year==2006)\n\nrd.result1 <- rdplot(y=final.bw1$ln_share, \n                     x=final.bw1$LISPremium, \n                     c=0,\n                     p=4,\n                     hide=TRUE,\n                     ci=95,\n                     nbins=20)\n\nbin.avg1 <- as_tibble(rd.result1$vars_bins)\n\nrd.plot1 <- bin.avg1 %>%\n  ggplot() +\n  geom_point(aes(x=rdplot_mean_bin, y=rdplot_mean_y)) +\n  geom_vline(aes(xintercept=0),linetype='dashed') +\n  labs(\n    x=\"Monthly premium - LIS subsidy, 2006\",\n    y=\"log enrollment share, 2006\"\n  ) +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_l), linetype='longdash') +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_r), linetype='longdash') +\n  theme_bw()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Binned Scatterplots with Different Binwidths\"}\nrd.result2 <- rdplot(y=final.bw1$ln_share, \n                     x=final.bw1$LISPremium, \n                     c=0,\n                     p=4,\n                     hide=TRUE,\n                     ci=95,\n                     nbins=10)\n\nbin.avg2 <- as_tibble(cbind(rd.result2$vars_bins))\n\nrd.plot2 <- bin.avg2 %>%\n  ggplot() +\n  geom_point(aes(x=rdplot_mean_bin, y=rdplot_mean_y)) +\n  geom_vline(aes(xintercept=0),linetype='dashed') +\n  labs(\n    x=\"Monthly premium - LIS subsidy, 2006\",\n    y=\"log enrollment share, 2006\"\n  ) +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_l), linetype='longdash') +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_r), linetype='longdash') +\n  theme_bw()\n\n\n\nrd.result3 <- rdplot(y=final.bw1$ln_share, \n                     x=final.bw1$LISPremium, \n                     c=0,\n                     p=4,\n                     hide=TRUE,\n                     ci=95,\n                     nbins=30)\n\nbin.avg3 <- as_tibble(cbind(rd.result3$vars_bins))\n\nrd.plot3 <- bin.avg3 %>%\n  ggplot() +\n  geom_point(aes(x=rdplot_mean_bin, y=rdplot_mean_y)) +\n  geom_vline(aes(xintercept=0),linetype='dashed') +\n  labs(\n    x=\"Monthly premium - LIS subsidy, 2006\",\n    y=\"log enrollment share, 2006\"\n  ) +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_l), linetype='longdash') +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_r), linetype='longdash') +\n  theme_bw()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Scatterplot with Optimal Binwidth\"}\nrd.result4 <- rdplot(y=final.bw1$ln_share, \n                     x=final.bw1$LISPremium, \n                     c=0,\n                     p=4,\n                     hide=TRUE,\n                     ci=95,\n                     binselect=\"es\")\n\nbin.avg4 <- as_tibble(cbind(rd.result4$vars_bins))\n\nrd.plot4 <- bin.avg4 %>%\n  ggplot() +\n  geom_point(aes(x=rdplot_mean_bin, y=rdplot_mean_y)) +\n  geom_vline(aes(xintercept=0),linetype='dashed') +\n  labs(\n    x=\"Monthly premium - LIS subsidy, 2006\",\n    y=\"log enrollment share, 2006\"\n  ) +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_l), linetype='longdash') +\n  geom_line(aes(x=rdplot_mean_bin, y=rdplot_ci_r), linetype='longdash') +\n  theme_bw()\n\nopt.bins <- nrow(bin.avg4)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Manipulation Test\"}\nrd.test <- rddensity(X=final.bw1$LISPremium, p=4)\npval.rd.test <- rd.test$test$p_jk\ndiff.rd.test <- rd.test$hat$diff\nrd.test.plot <- rdplotdensity(rd.test, final.bw1$LISPremium,\n                              lcol = c(\"black\",\"black\"),\n                              xlabel = \"Monthly premium - LIS subsidy, 2006\",\n                              plotRange = c(-10,10),\n                              plotN = 100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"RD Estimation\"}\ndata.2006 <- final.data %>%\n  filter(year==2006) %>%\n  select(orgParentCode, planName, state, contractId, uniqueID,\n         enrollment, share, ln_share, LISPremium_2006=LISPremium,\n         premium_2006=premium)\n\n\nfor (i in 2006:2010) {\n  rd.dat <- final.data %>% filter(year==i) %>%\n    left_join(data.2006 %>% select(uniqueID, LISPremium_2006), by=c(\"uniqueID\"))\n  rd.est.p1 <- rdrobust(y=rd.dat$ln_share, x=rd.dat$LISPremium_2006, h=4, kernel=\"uniform\")\n  rd.est.p2 <- rdrobust(y=rd.dat$ln_share, x=rd.dat$LISPremium_2006, h=4, kernel=\"uniform\", p=2)\n  \n  ti <- data.frame(\n    term=c(\"RD Linear\", \"RD Polynomial\"),\n    estimate=c(-1*rd.est.p1$coef[1], -1*rd.est.p2$coef[1]),\n    std.error=c(rd.est.p1$se[1], rd.est.p2$se[1])\n  )\n  \n  gl <- data.frame(\n    N1 = rd.est.p1$M[1],\n    N2 = rd.est.p1$M[2]\n  )\n\n  mod <- list(\n    tidy=ti,\n    glance=gl\n  )\n\n  class(mod) <- \"modelsummary_list\"\n    \n  assign(paste0(\"mod\",i),mod)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"CE-optimal Bandwidth\"}\nfor (i in 2006:2010) {\n  rd.dat <- final.data %>% filter(year==i) %>%\n    left_join(data.2006 %>% select(uniqueID, LISPremium_2006), by=c(\"uniqueID\"))\n  rd.est.p1 <- rdrobust(y=rd.dat$ln_share, x=rd.dat$LISPremium_2006, bwselect=\"cerrd\", kernel=\"uniform\")\n  rd.est.p2 <- rdrobust(y=rd.dat$ln_share, x=rd.dat$LISPremium_2006, bwselect=\"cerrd\", kernel=\"uniform\", p=2)\n\n  ti <- data.frame(\n    term=c(\"RD Linear\", \"RD Polynomial\"),\n    estimate=c(-1*rd.est.p1$coef[1], -1*rd.est.p2$coef[1]),\n    std.error=c(rd.est.p1$se[1], rd.est.p2$se[1])\n  )\n  \n  gl <- data.frame(\n    N1 = rd.est.p1$M[1],\n    N2 = rd.est.p1$M[2]\n  )\n  \n  mod <- list(\n    tidy=ti,\n    glance=gl\n  )\n  \n  class(mod) <- \"modelsummary_list\"\n  \n  assign(paste0(\"modce\",i),mod)\n  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Discontinuity as IV\"}\ndata.2007 <- final.data %>%\n  filter(year==2007) %>%\n  select(orgParentCode, planName, state, contractId, uniqueID,\n         premium_2007=premium)\n\nreg.data <- data.2006 %>%\n  left_join(data.2007, by=c(\"orgParentCode\",\"planName\",\"state\",\n                            \"contractId\",\"uniqueID\")) %>%\n  mutate(premium_diff=premium_2007-premium_2006)\n\nmean.share <- round(as.numeric(reg.data %>% summarize(mean_share=mean(share, na.rm=TRUE))), 3)\ninertia <- feols(premium_diff~1 | ln_share~LISPremium_2006, data=reg.data)\n```\n:::\n\n\n# Exercise 4: Demand Estimation and Market Share Construction\n\n---\n\n## Question\n\nWhat is the role of market definition on market shares and demand estimates?\n\n\n---\n\n## Data\n\n1. Hospital Cost Report Information System\n2. Hospital Service Area Files\n\n---\n\n## Data management\n\n- Biggest issue is creating markets using the community detection algorithm\n- I did that for you with the 'hospital_markets' data due to time, but if you want to do it yourself, please take a look at my ongoing [Hospital Choice Project](https://github.com/imccart/hospital-delivery-choice)\n\n\n---\n\n## Market Definition\n\n\n**Every** analysis of competition requires some definition of the market. This is complicated in healthcare for several reasons:\n\n1. Hospital markets more local than insurance markets\n2. Hospitals are multi-product firms\n3. Geographic market may differ by procedure\n4. Insurance networks limit choice within a geographic market\n\n\n---\n\n## Hospital Service Areas (HSAs)\n\n1. Begin with town or cities with a hospital (possibly more than one)\n2. Assign zip codes to that town/hospital(s) if the plurality of people in that zip code receive care from that town/hospital(s)\n3. Define the HSA as all contiguous zip codes from step 2\n\nAround 3,400 HSAs total\n\n\n---\n\n## Hospital Referral Regions (HRRs)\n\n- Contiguous HSAs\n- Population of at least 120,000\n- Account for at least 65% of residents' health care services (cardiovascular and neurosurgery)\n\n306 HRRs total\n\n---\n\n## Community Detection\n\nGoal: Identify connected nodes (some geographic region like zip code or county) where residents tend to receive health care services\n\n---\n\n## Community Detection\n\n1. Form data on geographic units, providers, and patient counts (bipartite). This is a matrix with geographic unit as row, provider as columns, and patient counts as cells\n2. Convert to matrix on counts of connections (common hospitals) between geographic areas (unipartite)\n3. Employ \"cluster walktrap\" algorithm to identify clusters of geographic units\n\n---\n\n## Cluster Walktrap\n\nWhat is a \"cluster walktrap\"?\n\n- Identify densely connected subgraphs (aka communities) \n- Random walk\n    - \"walker\" moves from node to node, uniformly randomly among neighbors\n    - \"distance\" will be large across communities and small within a community\n- Algorithm\n    - Begin with each node as its own community and calculate distance from each adjacent node\n    - Merge two adjacent communities, selected based on minimum sum of squared distances between each node and its community\n    - Update distances between communities and repeat\n\n\n---\n\n## Estimation\n\n- Asked to estimate a discrete choice logit model, but we only have continuous data on market shares at the hospital level\n- So...we need to employ @berry1994\n- His paper shows that a logit discrete choice model can be estimated with continuous market share data as follows...\n\n\n---\n\n## Basic Setup\n\nIndirect utility of person $i$, $$u_{ij} = x_{ij}\\beta + \\epsilon_{ij},$$ where $x_{ij}$ denotes person (and perhaps product) characteristics and $\\epsilon_{ij}$ denotes an error term. \n\n- Standard logit: one choice, $j=0,1$\n- Multinomial logit: many possible choices, $j=0,1,...,J$\n\n\n---\n\n## Logit terminology\n\nA few different terms for very similar models:\n\n- Multinomial Logit: Individual covariates only, alternative-specific coeficients. \n$$u_{ij}=x_{i}\\beta_{j} + \\epsilon_{ij},$$ such that $$p_{ij} = \\frac{e^{x_{i}\\beta_{j}}}{\\sum_{k} e^{x_{i}\\beta_{k}}}$$\n\n- Conditional Logit: Allow for alternative-specific regressors, such that $$u_{ij}=x_{ij}\\beta + \\epsilon_{ij}$$\n- \"Mixed\" Logit: Allow for individual and alternative-specific regressors, such that $$u_{ij}=x_{ij}\\beta + w_{i} \\gamma_{j} + \\epsilon_{ij}$$\n\n::: {.fragment}\n*but* people sometimes use \"mixed\" to refer to random-coefficients logit\n:::\n\n---\n\n## Does it matter?\n\nThese are really all the same and it's just a matter of specification (e.g., interact individual covariates with product characteristics or with product dummies). I'll refer to them as \"multinomial\" logit.\n\n\n---\n\n## The Indepenence of Irrelevant Alternatives\n\nFundamental issue with logit models...the ratio of choice probabilities for $j$ and $k$ does not depend on any other alternatives: $$\\frac{P_{ij}}{P_{ik}} = \\frac{e^{V_{ij}}}{e^{V_{ik}}}.$$\n\n---\n\n## Relaxing IIA\n\n- This is really an omitted variables problem...with enough interactions, we can allow for a sufficiently rich substitution pattern\n- Alternatively, relax assumptions on the error term with nested logit or random-coefficient logit (or multinomial probit)\n\n---\n\n## Discrete choice with market level data\n\nUtility of individual $i$ from selecting product $j$ is $$U_{ij}=\\delta_{j}+\\epsilon_{ij},$$\nwhere $\\delta_{j}=x_{j}\\beta + \\xi_{j}$, and $\\xi_{j}$ represents the mean level of utility derived from unobserved characteristics.\n\n---\n\n## Discrete choice with market level data\n\nGoal is to find $\\hat{\\delta}$ to statisfy moment condition, $$\\frac{1}{J}\\sum_{j} (\\hat{\\delta}_{j}-x_{j}\\beta)z_{j}=0.$$\n\nIn standard logit, $s_{j}=e^{\\delta_{j}}/\\sum e^{\\delta_{j}}$, and $\\delta_{j}$ then follows directly from taking logs and subtracting the outside share (with the normalization of $\\delta_{0}=0$, which yields the estimating equation $$\\ln(s_{j}) - \\ln(s_{0}) = x_{j}\\beta + \\xi_{j}$$\n\n---\n\n## Discrete choice with market level data\n\n- Standard logit imposes cross-price elasticities that are proportional to market shares (limited substitution patterns)\n- Relax with nested logit or random-coefficients logit\n\n---\n\n## References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}